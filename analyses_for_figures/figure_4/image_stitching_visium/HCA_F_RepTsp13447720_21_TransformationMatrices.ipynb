{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a344fe0",
   "metadata": {},
   "source": [
    "## Stitch HCA_F_RepTsp13447720 & HCA_F_RepTsp13447721 (Uterus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb9f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init \n",
    "import glob\n",
    "import numpy as np\n",
    "import os, sys, re\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from skimage.transform import rescale, resize, downscale_local_mean,  warp, AffineTransform\n",
    "from PIL import Image, ImageDraw, ImageFilter\n",
    "from skimage import measure, img_as_ubyte, color \n",
    "from skimage.measure import ransac\n",
    "import pandas as pd\n",
    "import math \n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import cv2\n",
    "# from cellpose import utils, io, models, plot\n",
    "from numpy import genfromtxt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63d0d3",
   "metadata": {},
   "source": [
    "### 1. Input images \n",
    "\n",
    "**tissue_hires_image.png** : this is a downsampled version of the original, full-resolution image provided by the user to spaceranger. Downsampling is obtained by box filtering, which averages RGB values in patches of pixels in the full resolution images to obtain an RGB value of one pixel in the downsampled image. Downsampled images maintain the aspect ratio of the original image. \n",
    "\n",
    " - For Visium slides of 6.5mm capture are, the **tissue_hires_image.png** has 2000 pixels in its largest dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3fc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed8d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.MAX_IMAGE_PIXELS = 20000000000\n",
    "os.chdir('/home/jovyan/RepTract/SPATIAL_ANALYSIS/ImageStitching/')\n",
    "imgP1 = Image.open('OriginalImages/20_tissue_hires_image.png')\n",
    "img1 = np.array(imgP1)\n",
    "print(\"Shape of image 1 is: {}\".format(img1.shape))\n",
    "imgP2 = Image.open('OriginalImages/21_tissue_hires_image.png')\n",
    "img2 = np.array(imgP2)\n",
    "print(\"Shape of image 2 is: {}\".format(img2.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are a result from the TrakEM plugin in Fiji and can be found in the project .xml file that is generated \n",
    "\n",
    "matrix1 = [[0.6191105586788903,-0.1621792715864541],[0.1621792715864541,0.6191105586788903],[241.2867845405894,754.6996793405884]]\n",
    "matrix2 = [[0.6400000000000001,0.0],[0.0,0.6400000000000001],[750.8799999999999,807.3999999999999]]\n",
    "\n",
    "M1 = np.asmatrix(np.array(matrix1)).T\n",
    "M2 = np.asmatrix(np.array(matrix2)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eae418",
   "metadata": {},
   "source": [
    "### 2. Affine transformation\n",
    "\n",
    "An affine transformation is any transformation that preserves collinearity, parallelism as well as the ratio of distances between the points (e.g. midpoint of a line remains the midpoint after transformation). It doesn’t necessarily preserve distances and angles.\n",
    "\n",
    "Translation, rotation, scaling, etc are all affine transformations as all the above properties are preserved in these transformations. To understand in simple terms, one can think of the affine transformation as a composition of rotation, translation, scaling, and shear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540a9fa",
   "metadata": {},
   "source": [
    "#### Usage of cv2.warpAffine() \n",
    "\n",
    "= function that applies an affine transformation to an image\n",
    "\n",
    "dst = cv2.warpAffine(*src*, *M*, *dsize*)\n",
    " \n",
    " - *src*: input image\n",
    " - *M*: Transformation matrix\n",
    " - *dsize*: size of the output image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448181e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst1 = cv2.warpAffine(img1,M1,(2100,2100))\n",
    "dst2 = cv2.warpAffine(img2,M2,(2100,2100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(dst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(dst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c693b",
   "metadata": {},
   "source": [
    "### 3. Create masks around the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd81ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowbound = np.array([1, 1, 1, 0])\n",
    "upbound = np.array([255, 255, 255, 255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4441cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = cv2.inRange(dst1, lowbound, upbound)\n",
    "plt.figure(dpi=200)\n",
    "plt.imshow(mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask2 = cv2.inRange(dst2, lowbound, upbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(mask2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df7a54c",
   "metadata": {},
   "source": [
    "### 4. Get the overlap between the two masks \n",
    "\n",
    "The reason we do the operation on masks is that the **bitwise_and** operation calculates the per-element bit-wise logical conjunction of two arrays and applying directly to the image gives funky results. The formula is:\n",
    "\n",
    "dst(I) = src1(I) ∧ src2(I) if mask(I) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d1c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = cv2.bitwise_and(mask1, mask2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_overlap = cv2.bitwise_xor(mask1, mask2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea0788",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb002033",
   "metadata": {},
   "source": [
    "### 5. Blend the images so that we get the same level of transparency for the whole image \n",
    "\n",
    "Haven't figured out how to do this elegantly, but because I haven't found a function in openCV that simply scales an image I am using **.addWeighted** of an image with itself to set the transparency of each image at half of its original value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial1 = cv2.addWeighted(dst1, 0.5, dst1, 0, 0)\n",
    "partial2 = cv2.addWeighted(dst2, 0.5, dst2, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa4bde",
   "metadata": {},
   "source": [
    "Now that both input images are set to half their original level of transparency, add them in the overlapping region so that it gets the original transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f71dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_res_overlap = cv2.add(partial1, partial2, mask = overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03842d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(partial_res_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa02ca",
   "metadata": {},
   "source": [
    "Add the original images in their non-overlapping regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9fe024",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_res_no_overlap = cv2.add(dst1, dst2, mask = ~overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(partial_res_no_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f37327",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cv2.add(partial_res_overlap, partial_res_no_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b49ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775489ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=400)\n",
    "plt.imshow(result)\n",
    "res1 = Image.fromarray(dst1)\n",
    "res1.save('20_tissue_hires_image.png')\n",
    "res2 = Image.fromarray(dst2)\n",
    "res2.save('21_tissue_hires_image.png')\n",
    "res = Image.fromarray(result)\n",
    "res.save('20_21_tissue_hires_image.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ecdeeb",
   "metadata": {},
   "source": [
    "### 6. From pixels to spots \n",
    "\n",
    " - **scalefactors_json.json** file contains the following fields: \n",
    "     - *tissue_hires_scalef* = a scaling factor that converts pixel positions in the original, full-resolution image to pixel positions in the **tissue_hires_image.png**\n",
    "     - *fiducial_diameter_fullres* = the number of pixels that span the diameter of a fiducial spot in the original, full resolution image \n",
    "     - *spot_diameter_fullres* = the number of pixels that span the diameter of a theoretical 65um spot in the original, full-resolution image \n",
    "     \n",
    " - **tissue_positions.csv** file contains a table with rows that correspond to spots (! this file used to be called tissue_positions_list.csv and has changed names in spaceranger v2.0. I am using spaceranger v2.0 but have renamed the file otherwise scanpy is unable to read the object). Excluding the header, the file has 4992 rows for Visium slides with 6.5 mm capture area and it is the number of spots in the spatial array. The columns correspond to the following fields: \n",
    "     - *barcode* = sequence barcode associated to the spot \n",
    "     - *in_tissue* = binary, indicating if spot falls inside (1) or outside (0) the tissue \n",
    "     - *array_row* = for Visium slide 6.5 mm capture are, the row coordinate of the spot in the array from 0 to 77 \n",
    "     - *array_col* = the column coordinate of the spot in the array. In order to express the \"orange crate\" arrangement of the spots, for Visium slide 6.5 mm catpure area this column index uses even numbers from 0 to 126 on even row and odd numbers from 1 to 127 for odd rows with each row resulting in 64 spots. \n",
    "     - *pxl_row_in_fullres* = the row pixel coordinates of the center of the spot in the full resolution image \n",
    "     - *pxl_col_in_fullres* = the column pixel coordinates of the center of the spot in the full resolution image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "spots1 = pd.read_csv('/nfs/team292/vl6/FetalReproductiveTract/VISIUM/data/HCA_F_RepTsp13447720/spatial/tissue_positions_list.csv',header=None)\n",
    "f1 = open('/nfs/team292/vl6/FetalReproductiveTract/VISIUM/data/HCA_F_RepTsp13447720/spatial/scalefactors_json.json'); data1 = json.load(f1); scale1 = data1['tissue_hires_scalef'] \n",
    "spots2 = pd.read_csv('/nfs/team292/vl6/FetalReproductiveTract/VISIUM/data/HCA_F_RepTsp13447721/spatial/tissue_positions_list.csv',header=None)\n",
    "f2 = open('/nfs/team292/vl6/FetalReproductiveTract/VISIUM/data/HCA_F_RepTsp13447721/spatial/scalefactors_json.json'); data2 = json.load(f2); scale2 = data2['tissue_hires_scalef'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d3677",
   "metadata": {},
   "outputs": [],
   "source": [
    "spots1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8808c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1['tissue_hires_scalef'])\n",
    "print(data2['tissue_hires_scalef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Visium spot matrix by taking column and row pixel coordinates scaled by their scalefactors \n",
    "sp1 = np.asmatrix(np.vstack((np.array(spots1.iloc[:,5].values*scale1),np.array(spots1.iloc[:,4].values*scale1),np.ones(spots1.iloc[:,4].values.shape[0]))))\n",
    "# Correct shape of transformation matrix \n",
    "M11 = np.append(M1, [[0,0,1]], axis = 0)\n",
    "# Apply transformation matrix \n",
    "sp1TTarget = np.matmul(M11,sp1)\n",
    "\n",
    "\n",
    "# Create the Visium spot matrix by taking column and row pixel coordinates scaled by their scalefactors \n",
    "sp2 = np.asmatrix(np.vstack((np.array(spots2.iloc[:,5].values*scale2),np.array(spots2.iloc[:,4].values*scale2),np.ones(spots2.iloc[:,4].values.shape[0]))))\n",
    "# Correct shape of transformation matrix \n",
    "M22 = np.append(M2, [[0,0,1]], axis = 0)\n",
    "# Apply transformation matrix \n",
    "sp2TTarget = np.matmul(M22,sp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d7a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1TTarget.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2TTarget.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74670dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct overlap and record the corresponding spots in both images\n",
    "corresponding_spots = {}\n",
    "for p in range(sp1TTarget[0].size):\n",
    "    sum_array = np.sum(np.abs(sp1TTarget[:-1,:]-sp2TTarget[:-1,p]),axis=0) # Remove the last dimension of all 1s and compute distance\n",
    "    min_dist = np.min(sum_array) \n",
    "    min_dist_indices = np.where(sum_array == min_dist)\n",
    "    assert len(min_dist_indices) == 2 # Make sure there is only one spot in image 1 that has minimal distance to the spot we are iterating in image 2\n",
    "    if min_dist<10:\n",
    "        # spots2.iloc[p,1] = 0 # Set the \"in_tissue\" value to 0 for the overlapping spots in image 2\n",
    "        corresponding_spots[min_dist_indices[1][0]] = p # Record correspondence of overlapping spots \n",
    "        \n",
    "        \n",
    "color = [(spots1.iloc[:,1].values).tolist(),(spots2.iloc[:,1].values*2).tolist()]\n",
    "plt.figure(dpi=600)\n",
    "plt.scatter([sp1TTarget[0,:].tolist(),sp2TTarget[0,:].tolist()],[sp1TTarget[1,:].tolist(),sp2TTarget[1,:].tolist()],s=0.1,c=color,alpha=0.7,facecolor=None)\n",
    "plt.axis('equal')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spots11 = spots1.copy()\n",
    "spots22 = spots2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "spots22.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc81a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct coordinates\n",
    "spots11.iloc[:,-2:] =  (sp1TTarget[:-1,:].T*(1/scale1)).tolist()\n",
    "spots22.iloc[:,-2:] =  (sp2TTarget[:-1,:].T*(1/scale2)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ce2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spots22.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure everything is ok \n",
    "color = [(spots11.iloc[:,1].values).tolist(),(spots22.iloc[:,1].values*2).tolist()]\n",
    "plt.figure(dpi=400)\n",
    "plt.scatter([spots11.iloc[:,4].values.tolist(),spots22.iloc[:,4].values.tolist()],[spots11.iloc[:,5].values.tolist(),spots22.iloc[:,5].values.tolist()],s=0.1,c=color,alpha=0.7,facecolor=None)\n",
    "plt.axis('equal')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ae682",
   "metadata": {},
   "outputs": [],
   "source": [
    "spots11[0] = [\"HCA_F_RepTsp13447720_\"+i for i in spots11[0]]\n",
    "spots22[0] = [\"HCA_F_RepTsp13447721_\"+i for i in spots22[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba39a24",
   "metadata": {},
   "source": [
    "### Save the overlapping barcodes as .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad21679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_barcodes = {}\n",
    "for k, v in corresponding_spots.items():\n",
    "    overlapping_barcodes[spots11.iloc[k,0]] = spots22.iloc[v,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(overlapping_barcodes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960214fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(list(overlapping_barcodes.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8c6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"20_21_overlapping_barcodes.json\", \"w\") as f:\n",
    "    json.dump(overlapping_barcodes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2cfb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "spots11.to_csv('20_tissue_positions_list.csv',header=False,index=False)\n",
    "spots22.to_csv('21_tissue_positions_list.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c41f2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bccdcec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagespot",
   "language": "python",
   "name": "imagespot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
